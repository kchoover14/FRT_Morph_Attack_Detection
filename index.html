<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kara C. Hoover, PhD">
<meta name="dcterms.date" content="2025-02-01">

<title>Morphing Attack Detection for Identity Documents</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="morphSummary_files/libs/clipboard/clipboard.min.js"></script>
<script src="morphSummary_files/libs/quarto-html/quarto.js"></script>
<script src="morphSummary_files/libs/quarto-html/popper.min.js"></script>
<script src="morphSummary_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="morphSummary_files/libs/quarto-html/anchor.min.js"></script>
<link href="morphSummary_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="morphSummary_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="morphSummary_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="morphSummary_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="morphSummary_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


<link rel="stylesheet" href="styles.css">
</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive Summary</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#overall-performance" id="toc-overall-performance" class="nav-link" data-scroll-target="#overall-performance">Overall Performance</a></li>
  </ul></li>
  <li><a href="#training-performance" id="toc-training-performance" class="nav-link" data-scroll-target="#training-performance">Training Performance</a></li>
  <li><a href="#detection-performance" id="toc-detection-performance" class="nav-link" data-scroll-target="#detection-performance">Detection Performance</a></li>
  <li><a href="#what-the-model-learned" id="toc-what-the-model-learned" class="nav-link" data-scroll-target="#what-the-model-learned">What the Model Learned</a></li>
  <li><a href="#example-morphed-images" id="toc-example-morphed-images" class="nav-link" data-scroll-target="#example-morphed-images">Example Morphed Images</a></li>
  <li><a href="#comparison-to-state-of-the-art" id="toc-comparison-to-state-of-the-art" class="nav-link" data-scroll-target="#comparison-to-state-of-the-art">Comparison to State-of-the-Art</a></li>
  <li><a href="#operational-implications" id="toc-operational-implications" class="nav-link" data-scroll-target="#operational-implications">Operational Implications</a></li>
  <li><a href="#technical-insights" id="toc-technical-insights" class="nav-link" data-scroll-target="#technical-insights">Technical Insights</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Morphing Attack Detection for Identity Documents</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Kara C. Hoover, PhD </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="executive-summary" class="level2">
<h2 class="anchored" data-anchor-id="executive-summary">Executive Summary</h2>
<p><strong>Background.</strong> Morphing attacks are a critical threat to passport issuance systems. By blending two people’s faces into a single image, attackers create documents that pass face recognition checks for both individuals—enabling identity fraud at borders and security checkpoints. IDSL tests commercial morph detection systems for government clients.</p>
<p><strong>Goal:</strong> Train a binary classifier to distinguish genuine face images from morphed (blended) images and understand presentation attack detection workflows.</p>
<p><strong>Approach:</strong> Downloaded 500 faces from Labeled Faces in the Wild dataset, generated 250 morphed images using alpha blending, trained ResNet50-based classifier using transfer learning, and evaluated performance with Grad-CAM visualizations to understand model decision-making.</p>
<p><strong>Results:</strong> Achieved 93% test accuracy with 98% precision and 89% recall on morphed images. Model correctly identified all genuine faces (100% genuine recall) but missed 11% of morphs. Grad-CAM analysis revealed the model struggles with blur, angled faces, and facial hair.</p>
<hr>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p><strong>Model architecture:</strong> ResNet50 (pre-trained on ImageNet), transfer learning approach with frozen early layers and fine-tuned final 20 layers, custom classifier head (2048 features → 512 → 2 classes).</p>
<p><strong>Training configuration:</strong> Platform: Google Colab (Tesla T4 GPU); Optimizer: Adam (lr=0.0001); Epochs: 10; Batch size: 32; Data split: 60% train / 20% val / 20% test.</p>
<p><strong>Why transfer learning?</strong> Pre-trained ResNet already knows how to extract facial features from millions of ImageNet images. We adapt the final layers to detect morphing artifacts rather than training from scratch (which would require 10,000+ images and days of training).</p>
<hr>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="overall-performance" class="level3">
<h3 class="anchored" data-anchor-id="overall-performance">Overall Performance</h3>
<p>Test set accuracy: 93%</p>
<p>Precision (morphed class): 98% - when model flags something as morphed, it’s right 98% of the time</p>
<p>Recall (morphed class): 89% - detects 89% of actual morphs</p>
<p>Confusion matrix: All 46 genuine faces correctly identified (100%); 48 of 54 morphs detected (89%); 6 morphs missed (11% false negative rate)</p>
<p><strong>Interpretation:</strong> The model is conservative toward genuine faces—it rarely false-alarms on legitimate photos. This is operationally appropriate for passport issuance where false rejecting a genuine applicant creates inconvenience but false accepting a morphed photo creates security breach. The 11% miss rate on morphs would be concerning for high-security deployment but acceptable for secondary screening where human review catches the remainder.</p>
<hr>
</section>
</section>
<section id="training-performance" class="level2">
<h2 class="anchored" data-anchor-id="training-performance">Training Performance</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="training_curves.png" class="img-fluid figure-img"></p>
<figcaption>Training and Validation Curves</figcaption>
</figure>
</div>
<p>Both training and validation loss decrease steadily with validation accuracy plateauing around 93% after epoch 5. No significant gap between training and validation suggests good generalization. The model converges quickly, suggesting pre-trained ResNet features are highly transferable to morph detection.</p>
<hr>
</section>
<section id="detection-performance" class="level2">
<h2 class="anchored" data-anchor-id="detection-performance">Detection Performance</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="roc_curve.png" class="img-fluid figure-img"></p>
<figcaption>ROC Curve</figcaption>
</figure>
</div>
<p>Area Under Curve (AUC) = 0.9935—near-perfect discrimination. At threshold 0.5, the model achieves 89% true positive rate with 0% false positive rate. We could lower threshold to catch more morphs (~95% detection) but this would increase false alarms on genuine faces (~5%). The decision depends on use case: primary gate vs.&nbsp;secondary screening.</p>
<hr>
</section>
<section id="what-the-model-learned" class="level2">
<h2 class="anchored" data-anchor-id="what-the-model-learned">What the Model Learned</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="gradcam_visualizations.png" class="img-fluid figure-img"></p>
<figcaption>Grad-CAM Attention Maps</figcaption>
</figure>
</div>
<p>Grad-CAM (Gradient-weighted Class Activation Mapping) reveals where the model looks when making decisions. Red/yellow regions show high attention; blue shows low attention.</p>
<p><strong>For genuine faces (correctly classified):</strong> Model attends to eyes, nose, mouth (natural facial features) with attention distributed across face, focusing on high-contrast features.</p>
<p><strong>For morphed faces (correctly detected):</strong> Model attends to edges of face and hair boundaries, focuses on texture transitions where blending artifacts appear, and examines skin texture regions where blending creates unnatural smoothness.</p>
<p><strong>Observed challenges:</strong> Blur—morphing process adds smoothing, so blurred genuine images can look morphed. Angles—profile or angled faces provide fewer frontal features for analysis. Facial hair—beards create texture complexity that can mask blending artifacts.</p>
<p><strong>Operational implication:</strong> Image quality matters. The model struggles when source images are blurred/low resolution, faces are not frontal (&gt;15° rotation), or heavy facial hair obscures structure. Recommendation: implement quality checks before morph detection (minimum 600x600 pixels, frontal pose ±10°, sharpness threshold, lighting uniformity).</p>
<hr>
</section>
<section id="example-morphed-images" class="level2">
<h2 class="anchored" data-anchor-id="example-morphed-images">Example Morphed Images</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="morph_examples.png" class="img-fluid figure-img"></p>
<figcaption>Genuine vs Morphed Examples</figcaption>
</figure>
</div>
<p>Top row shows genuine faces from LFW; bottom row shows morphed images (50/50 blend of two randomly selected faces). Even to human eyes, some morphs are difficult to distinguish—especially when both source faces have similar skin tones, ages, and aligned features. This demonstrates why automated detection is necessary; human reviewers cannot reliably detect morphs without specialized training.</p>
<hr>
</section>
<section id="comparison-to-state-of-the-art" class="level2">
<h2 class="anchored" data-anchor-id="comparison-to-state-of-the-art">Comparison to State-of-the-Art</h2>
<p>Published benchmarks for simple alpha-blended morphs (like ours) show academic systems achieving 90-95% detection. Our 93% result is comparable. However, more sophisticated attacks perform worse: landmark-based morphs (75-85% academic detection, expect 65-75% for our model) and GAN-generated morphs (60-70% academic detection, expect 50-60% for our model). For operational deployment, systems must be tested against the most sophisticated morphing techniques attackers actually use—not just simple alpha blending.</p>
<hr>
</section>
<section id="operational-implications" class="level2">
<h2 class="anchored" data-anchor-id="operational-implications">Operational Implications</h2>
<p><strong>Strengths:</strong> 93% detection adequate for secondary screening; high precision (98%) means few false alarms; fast inference (~50ms per image with GPU); explainable via Grad-CAM (builds operator trust); transfer learning requires relatively small training dataset (500 images) and trains quickly (10 epochs, ~15 minutes).</p>
<p><strong>Limitations:</strong> 11% miss rate on simple morphs unacceptable for high-security primary gate; only tested on alpha-blended morphs (not landmark-based or GAN); single model (no ensemble for robustness); no demographic fairness analysis; image quality dependencies (struggles with blur, angles, facial hair).</p>
<p><strong>Recommended deployment:</strong> Use as secondary screening tool in a tiered system: (1) Primary check: document security features (holograms, UV, microprint); (2) Quality filter: automated checks for resolution, pose, lighting, sharpness; (3) Morph detection: this model flags suspicious images; (4) Human review: expert examines flagged cases. Not recommended as primary automated gate without human review or deployment without demographic fairness testing.</p>
<hr>
</section>
<section id="technical-insights" class="level2">
<h2 class="anchored" data-anchor-id="technical-insights">Technical Insights</h2>
<p><strong>Transfer learning is powerful.</strong> ResNet50 pre-trained on ImageNet already knows facial features. Fine-tuning final layers achieved 93% accuracy with only 300 training images (small by deep learning standards). Training from scratch would need 10,000+ images and take days/weeks.</p>
<p><strong>Grad-CAM provides actionable insights.</strong> Visualization confirms model attends to blending artifacts (edges, textures) rather than spurious correlations (backgrounds). It identifies failure modes (blur, angles, facial hair) and guides operational requirements (image quality standards). For IDSL work, explainability matters—clients need to understand why a system flags certain images.</p>
<p><strong>The threshold isn’t neutral.</strong> Current threshold (0.5) prioritizes precision over recall (zero false alarms, misses 11% of morphs). Alternative operating points exist: threshold 0.3 catches 95% of morphs with 5% false alarm rate; threshold 0.7 catches 80% with &lt;1% false alarms. Decision depends on context—passport issuance accepts higher false alarms to catch more morphs; border crossing accepts some misses to minimize throughput delays.</p>
<hr>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>